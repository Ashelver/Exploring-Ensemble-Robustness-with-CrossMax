% ----------------- CHAPTER 2 -----------------
\chapter{Literature Review}

\section{Empirical Adversarial Defenses}
\indent

Empirical adversarial defenses aim to improve robustness by training models directly on adversarially perturbed inputs. While adversarial training has proven highly effective in practice, its robustness is inherently bounded by the strength and diversity of attacks used during training, and models often remain vulnerable to unseen or stronger adversaries.

One of the most widely adopted techniques is \textit{adversarial training}, introduced by Madry et al.~\cite{madry2017towards}, which frames robustness as a min-max optimization problem. In this formulation, the model is trained on adversarial examples crafted using Projected Gradient Descent (PGD), making it robust against a strong first-order adversary. This approach has become a de facto standard for empirical robustness due to its effectiveness across various datasets and architectures.

Several improvements to adversarial training have since been proposed. For instance, TRADES~\cite{zhang2019theoretically} introduces a regularization term to explicitly control the trade-off between clean accuracy and adversarial robustness. MART~\cite{wang2019improving} emphasizes misclassified examples during training to improve robustness further. These methods demonstrate that empirical robustness can be systematically enhanced through better loss design and training strategies.

Despite their success, empirical methods face several limitations. First, they offer no formal guarantees: robustness is only evaluated against specific attacks used during training and testing. As a result, these defenses can remain vulnerable to stronger or adaptive attacks~\cite{tramer2020adaptive}. Second, adversarial training often incurs substantial computational cost and can lead to drops in clean accuracy~\cite{rice2020overfitting}. Moreover, empirical defenses tend to be sensitive to hyperparameter choices and may not generalize well across different threat models or perturbation budgets.

Nonetheless, empirical defenses remain a practical choice for real-world deployment due to their scalability and relatively high robustness under standard evaluations. However, adversarial training often requires substantial computational resources and may not generalize well across unseen attack patterns. In contrast, our study explores an alternative path to robustness by investigating the behavior of naturally trained models and their ensembles under strong adversarial attacks. Rather than relying on specialized training procedures, we focus on how ensemble diversity and score-level aggregation can contribute to adversarial robustness, offering a complementary perspective to conventional empirical defenses.


\section{Provable Adversarial Defenses}

\indent

Empirical adversarial defenses, such as adversarial training~\cite{madry2017towards}, improve robustness by exposing models to adversarial examples generated through specific attack algorithms. Provable defenses~\cite{wong2018provable, cohen2019certified} aim to offer formal guarantees that a model's prediction remains unchanged under all perturbations within a specified norm-bound (e.g., $\ell_\infty$ or $\ell_2$ balls), constructing mathematical certificates of robustness to ensure resilience against any perturbation within the threat model.

Within the landscape of certified robustness, existing approaches can be broadly categorized into complete, incomplete relaxation-based, and probabilistic approaches. Complete methods provide exact guarantees but rarely scale beyond small networks, while incomplete methods trade off exactness for tractability, and probabilistic approaches certify robustness in expectation under random noise distributions.


Complete methods aim to exhaustively verify the robustness of neural networks by exploring the entire input space within a perturbation set. Mixed-Integer Linear Programming (MILP) is one representative approach, where adversarial robustness is encoded as a set of linear constraints with integer variables for ReLU activations~\cite{tjeng2017evaluating}. Other complete verification techniques include satisfiability modulo theory (SMT)-based solvers~\cite{ehlers2017formal}, which encode network decisions into logical constraints, and Reluplex~\cite{katz2017reluplex}, an extension of the Simplex algorithm tailored to piecewise-linear activations. More recently and branch-and-bound frameworks~\cite{bunel2018unified} combine symbolic propagation with search to improve scalability. Despite their theoretical appeal and ability to provide exact robustness guarantees, these methods suffer from severe computational complexity, restricting their applicability to small networks or shallow architectures. Consequently, complete methods are often viewed as valuable for benchmarking and understanding robustness, but impractical for large-scale deployment.

In contrast, incomplete methods such as Linear Programming (LP)-based methods introduced by Wong and Kolter~\cite{wong2018provable} offer a more scalable alternative by relaxing the ReLU nonlinearity using convex outer approximations. By enabling efficient bound propagation through the network, this relaxation allows robustness training and certification to scale to larger models. The core idea in LP-based training is to optimize a convex upper bound on the worst-case loss within the perturbation set. At training time, models are encouraged to give correct prediction even under all adversarial perturbations. While this method provides certified robustness during training, it introduces a trade-off: increasing the tightness of the convex bounds typically reduces clean accuracy, since the additional constraints restrict the model’s capacity to fit clean data. Moreover, achieving tighter relaxations requires solving more complex optimization problems, which significantly limits scalability to deeper or more complex architectures. To obtain even lower certified bounds on adversarial loss, another incomplete approach within certified robustness employs semidefinite programming (SDP). In particular, Raghunathan et al.~\cite{raghunathan2018semidefinite} proposed an SDP-based relaxation to certify the robustness of ReLU networks. By formulating the adversarial perturbation problem as a quadratic program over the input, the authors derived an SDP relaxation that provides tighter certified bounds on adversarial loss than linear methods (e.g., LP), especially for small networks. However, due to the high computational cost of SDP solvers, these methods do not scale well to larger architectures or datasets. This trade-off between tightness and scalability remains a key challenge in certified robustness. To improve efficiency at the cost of looser certificates, Interval Bound Propagation (IBP)~\cite{gowal2018effectiveness} propagates input intervals through the network to estimate bounds on output perturbations. Though IBP is significantly more scalable, its bounds are often too loose to guarantee robustness in challenging scenarios. More recent work has attempted to combine the advantages of LP and IBP to balance tightness and computational efficiency~\cite{zhang2019towards}. In this project, we select LP-based certified training as the representative method because it offers a practical balance between rigor and scalability. Compared to more computationally expensive approaches such as SDP, LP certification can be applied at reasonable cost to non-trivial architectures on CIFAR-10, where it has already been successfully demonstrated~\cite{wong2018scaling}. This makes LP-based methods a widely adopted benchmark in certified robustness research and a suitable choice for our comparative study of ensembles, where both clean accuracy and robustness must be jointly examined.


Other approaches, such as randomized smoothing~\cite{cohen2019certified}, offer probabilistic robustness guarantees by averaging predictions over random noise injections. One key advantage is that randomized smoothing is architecture-agnostic and scales effectively to large datasets such as ImageNet, without requiring modifications to the underlying model. It is also simple to implement, since it only requires injecting noise during training and inference. Moreover, unlike purely empirical defenses, randomized smoothing provides certified guarantees in a probabilistic sense, which makes it an attractive practical compromise between scalability and formal robustness. However, its guarantees are typically restricted to $\ell_2$-norm perturbations and inference requires many stochastic forward passes, which can increase computational cost.

Despite their theoretical appeal, provable defenses remain constrained in practice. They often struggle to scale to large datasets like ImageNet or architectures with high capacity. Moreover, many provable training methods (e.g., LP-based or IBP-based) suffer from loose bounds when facing higher threat perturbation budgets, especially in high-dimensional input spaces.


In our study, we use LP-based convex relaxation as a certified robustness method. We adopt the convex relaxation framework introduced by Wong and Kolter~\cite{wong2018provable} to train robust base models, which are then integrated into ensemble systems. This allows us to investigate how certified training contributes to ensemble-level robustness, and whether ensembles of provably robust models retain their guarantees or exhibit new emergent behavior when combined under ensemble mechanisms like CrossMax~\cite{fort2024ensemble}.




\section{Ensemble Methods for Robustness}
\indent

Ensemble learning is a widely adopted technique in machine learning to enhance generalization by aggregating predictions from multiple models. In adversarial settings, ensembles can provide improved robustness by diversifying model behaviors, making it more difficult for a single perturbation to simultaneously fool all models. Classical ensemble strategies include majority voting and soft (logit) averaging, both of which have been investigated in adversarial contexts.

Tramèr et al.~\cite{tramer2017ensemble} systematically analyzed ensemble adversarial training, showing that ensembles of independently trained models can offer improved robustness. However, they also revealed that shared vulnerabilities among ensemble members can lead to a lack of diversity, making the ensemble susceptible to adaptive attacks. Strauss et al.~\cite{strauss2017ensemble} compared simple ensemble aggregation methods and observed that improvements under white-box attacks were marginal unless model diversity was explicitly enforced.

To address the lack of diversity in models, later works proposed diversity-promoting training objectives. Pang et al.~\cite{pang2019improving} introduced adversarially robust ensembles by maximizing disagreement among member models under clean and adversarial inputs. Similarly, Kariyappa and Qureshi~\cite{kariyappa2019improving} enforced orthogonality in input gradients across models to encourage diverse local decision boundaries. These methods reflect a growing awareness that ensemble effectiveness under adversarial threat relies not just on the number of models, but on the diversity and independence of their decision-making processes.

However, while some recent works have explored certified robustness in ensembles through training-time strategies, such as weighted optimization~\cite{zhang2019enhancing} or diversity regularization~\cite{yang2021certified}, the use of certified training methods such as LP-based approaches~\cite{wong2018provable} remains underexplored in the ensemble setting. In particular, there is limited investigation into purely inference-time ensemble aggregation for certified robustness, which motivates our study.

\textit{CrossMax}, introduced in the ``Ensemble Everything Everywhere'' framework~\cite{fort2024ensemble}, presents a novel aggregation strategy inspired by Vickrey auctions~\cite{vickrey1961counterspeculation}. It dynamically selects logits from intermediate layers based on cross-layer agreement, aiming to improve robustness through feature-level diversity. While originally applied to self-ensembling within a single model, its core idea naturally extends to ensembles composed of structurally different models, where architectural heterogeneity may offer further benefits. Later analysis by Zhang et al.~\cite{zhang2024evaluating} raised concerns about the robustness claims made in the original work, pointing to gradient masking as a potential confounding factor. Their findings highlighted the need for more rigorous evaluation.

Motivated by these insights, we extend CrossMax to a multi-model setting, where constituent networks differ in both architecture and training regime. This shift from the original self-ensemble design leverages model-level diversity to enhance robustness. Furthermore, in response to the concerns raised by~\cite{zhang2024evaluating} regarding potential gradient masking, we employ stronger, attack-aware evaluation protocols to ensure that any observed robustness gains cannot be attributed to such artifacts. While the robustness of CrossMax remains debated, its reliance on cross-model agreement makes it a candidate for further exploration in ensemble contexts. In this work, we extend CrossMax beyond self-ensembling and apply it across ensembles of independently trained models.




\section{Confidence and Robustness}
\indent

Recent advances in adversarial robustness have increasingly recognized the importance of not only achieving high accuracy but also reliably assessing the confidence of model predictions under adversarial perturbations. Confidence-aware methods seek to utilize uncertainty or disagreement signals within model predictions to better detect adversarial examples or to estimate prediction reliability~\cite{gal2016dropout}.

In the context of ensemble methods, confidence evaluation often involves analyzing inter-model agreement or prediction consistency as a proxy for robustness. Traditional ensemble aggregation techniques, such as majority voting or logits averaging, do not explicitly capture these uncertainty characteristics. To address this gap, novel approaches have been proposed to leverage intra-ensemble dynamics and disagreement patterns to infer adversarial risk~\cite{pang2019improving, abbasi2017robustness}.

In our study, we adapt CrossMax to a more general ensemble setting by applying it across a pool of independently trained models, rather than within a single network. Our adaptation of CrossMax allows it to act as a voting mechanism over logits selected from different independently trained models. Notably, this often results in not only a single predicted label but sometimes a \textit{prediction set} containing multiple plausible classes. This behavior was not discussed in the original CrossMax paper, but we find it particularly informative under adversarial conditions.

We define the prediction confidence of CrossMax as the proportion of samples for which it yields a unique prediction. Rather than interpreting higher singleton prediction rates as inherently better, we emphasize the stability of this confidence signal under adversarial perturbations. This insight introduces a novel perspective on ensemble confidence, where prediction set structure, rather than just softmax scores, serves as an interpretable, robust indicator of model reliability. Such confidence-aware outputs offer potential utility in downstream tasks such as adversarial example detection or adaptive defense triggering.